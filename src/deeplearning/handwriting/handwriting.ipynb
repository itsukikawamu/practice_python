{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ライブラリのインポート\n",
    "必要なデータを取得し、画像データをnp配列train_Xへ格納\n",
    "ラベルデータもtrain_yへ格納\n",
    "DLモデルを構築(今回はLeNet(初期の基礎的なCNNモデル)を学習させる)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0.jpg</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2.jpg</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3.jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4.jpg</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name  category_id\n",
       "0  train_0.jpg          5.0\n",
       "1  train_1.jpg          0.0\n",
       "2  train_2.jpg          4.0\n",
       "3  train_3.jpg          1.0\n",
       "4  train_4.jpg          9.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ラベルデータの読み込み\n",
    "train_master = pd.read_csv(\"train_master.tsv\", sep=\"\\t\")\n",
    "train_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width, height: (28, 28)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAQpZgqgkk4AHeuy0/4TeOdSXdB4duoxjP+kFYf/QyKs33wr1fSWu01TVdDsWtYRK6zXvzNnJ2KoBJbAB6Y5HNcFSg4ORXr3hfxvrfizw1qXhS58Ry2ur3DRy6fdzT+Wr7cAwlwMrkDI55OR354Dxfaa/ZeJbmLxL57aoNqySTHcZAoCqwb+IYUc965+tHStG1HXL1bTTLGa8nYgbIULYycZPoPc8V6hpnwj0/w9YLrHxE1aPTLbPyWULhpZD6ZGfyXP1FYHxL8c6b4tOkWOkWM0Gn6TCYYZblg0sq4UDPUjAXuTnOTXntaFhrWq6LNK2l6leWLv8rtbTtEWHoSpGajvr27v7kyXl1PcydN80hdvzP1NU6K/9k=",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACDklEQVR4AV1SPWgUQRT+Zmdm/y93lztiTmInSApTBTtNJWhlEQsLsbY4BDtrbdNqKoOdjU0gCjYRm0Ow0RQBO1MYCZoECXt7yd7e873ZuyP62J2dne997+d7A0ApbTXYlOIXMCGa8ivmedVXQPZzOCIrh3JQiouh0Yh4ExTGBMdKtgC7MlcZpTmCpzmss1RWF1QrGpL4UkkWs2gYVEyfQ1dZrZ/Ozbceb7zprBM9EabhhGoENVNbuHxlaWnBoPi2fqff6wko1esh0k6vFhD/nCn16Fd+cLYjYGWeDXepOKJsZ+s0twoBzASSRvzV5w+JPhksrmkEMRIHutqVZ6IWXv5+wL1fhAcnAXNEM9CozA/jo9a9JtJ9FaBwbXI5jgqEiPGerrFnWzQccyeZLZLl/t7rLpP4OVcQO2gbA9dpQN0Ox5oOY0xlbcPo6lvqv7jAgrNwbL4n2nMUI5G85D7RpjmXUrzCCEgCHbLko/3bCjUhwo+ZUY9l/pxp8dmH4+F32EoDho1cEoZSe2nt8KTM6R0fzgqRzYedgUpNo7tHGdHXm9G0j8QGMpumvtsrS8q2b9XFv+loUnOaorPx8yQn+rjaAOaM709EsAY3NndpMKSDp/NMCCS/0Lkar0B7ZaWef/n841WWQZenGAB/HAidcM7I58baHGs8hQpyq2ElOTVf0P9mMb35tdTTTP0H/gvaWYWytyqAWwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#画像データ\n",
    "image = Image.open(\"./train_images/train_0.jpg\")\n",
    "print('width, height:', image.size)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_id\n",
       "0.0    5923\n",
       "1.0    6742\n",
       "2.0    5958\n",
       "3.0    6130\n",
       "4.0    5842\n",
       "5.0    5421\n",
       "6.0    5918\n",
       "7.0    6265\n",
       "8.0    5851\n",
       "9.0    5949\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_master[\"category_id\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(train_master)\n",
    "image_size = (28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_Xは4次元の配列で初期値はすべて0(6000枚*1色* 28*28ピクセル)\n",
    "#ここにすべての画像データを1ピクセルごとに輝度を数値化して格納していく\n",
    "train_X = np.zeros( (num_train,) + (1,) + image_size)\n",
    "\n",
    "#train_yは画像データのラベルデータ(目的変数)\n",
    "train_y = np.zeros((num_train,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"./train_images\"\n",
    "image_files = [os.path.join(image_folder, fname) for fname in os.listdir(image_folder) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNが含まれているか: 1\n"
     ]
    }
   ],
   "source": [
    "#全ての画像とラベルを変換して配列に格納\n",
    "for data in train_master.iterrows(): \n",
    "    #iterrowsで(data[0]==index, data[1]==[画像データ名,ラベル(答え)])をタプルで取得する。\n",
    "    image = Image.open(image_files[data[0]])\n",
    "    image_array = np.array(image).reshape( (1,) + image_size)\n",
    "    train_X[data[0], :] = image_array / 255.\n",
    "    train_y[data[0]] = data[1][\"category_id\"]\n",
    "# NaNやinfがあるかチェック\n",
    "print(\"NaNが含まれているか:\", np.isnan(train_y).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kawam\\AppData\\Local\\Temp\\ipykernel_3424\\3913517390.py:3: RuntimeWarning: invalid value encountered in cast\n",
      "  train_y = train_y.astype(np.int32)\n"
     ]
    }
   ],
   "source": [
    "#モデル計算の効率を上げるため。多くのニューラルネットワークライブラリ（Chainer, PyTorch, TensorFlowなど）は float32 を推奨。\n",
    "train_X = train_X.astype(np.float32)\n",
    "train_y = train_y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DLモデルの構築。今回はLeNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニューラルネットワークLeNetのクラスを定義\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, out_size=10, act_func = \"\"):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 畳み込み層\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        # 全結合層\n",
    "        self.fc3 = nn.Linear(16 * 5 * 5, 120)  # 入力サイズ: 特徴マップサイズ (5x5) * チャネル数 (16)\n",
    "        self.fc4 = nn.Linear(120, 84)\n",
    "        self.fc5 = nn.Linear(84, out_size)\n",
    "        \n",
    "        # 活性化関数を設定\n",
    "        self.act_func = F.relu\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 順伝播\n",
    "        x = F.max_pool2d(self.act_func(self.conv1(x)), kernel_size=2, stride=2)  # 畳み込み1 + プーリング\n",
    "        x = F.max_pool2d(self.act_func(self.conv2(x)), kernel_size=2, stride=2)  # 畳み込み2 + プーリング\n",
    "        x = torch.flatten(x, 1)  # 平坦化\n",
    "        x = self.act_func(self.fc3(x))  # 全結合1\n",
    "        x = self.act_func(self.fc4(x))  # 全結合2\n",
    "        x = self.fc5(x)  # 出力層\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
